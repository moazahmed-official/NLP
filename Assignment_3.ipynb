{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moazahmed-official/NLP/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtvq6xnYQitb",
        "outputId": "b894bd30-9bfd-4adf-82f6-fe368c13e3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'customer-review-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/customer-review-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"parve05/customer-review-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/parve05/customer-review-dataset/versions/1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ7U5f4_Qwn5",
        "outputId": "4230588d-0158-4736-9f8e-5eec9593b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redmi6.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/root/.cache/kagglehub/datasets/parve05/customer-review-dataset/versions/1/redmi6.csv\" , encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "wFFEk-hVRIUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "fQGYC26G1MN9",
        "outputId": "d62e12bf-4964-4b5c-f9e9-0de9b0e80f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "def findTextColumn(columns):\n",
        "    lowerCols = [col.lower() for col in columns]\n",
        "    keywords = ['review', 'comments', 'comment', 'text', 'review text', 'reviewstext']\n",
        "    for keyword in keywords:\n",
        "        for i, col in enumerate(lowerCols):\n",
        "            if keyword in col:\n",
        "                return columns[i]\n",
        "    objCols = [col for col in columns if df[col].dtype == 'object']\n",
        "    if objCols:\n",
        "        avgLen = {col: df[col].dropna().astype(str).map(len).mean() for col in objCols}\n",
        "        return max(avgLen, key=avgLen.get)\n",
        "    return None\n",
        "\n",
        "textColumn = findTextColumn(df.columns.tolist())\n",
        "\n",
        "if textColumn is None:\n",
        "    raise KeyError(\"Couldn't find a text column automatically. Please pass it manually (e.g. 'Comments').\")\n",
        "\n",
        "print(\"Using text column:\", textColumn)\n",
        "\n",
        "df = df.copy()\n",
        "df.rename(columns={textColumn: 'reviewText'}, inplace=True)\n",
        "\n",
        "def preprocessText(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s']\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleanedText'] = df['reviewText'].apply(preprocessText)\n",
        "\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "def removeStopwords(tokens):\n",
        "    return [t for t in tokens if t not in stopWords and len(t) > 1]\n",
        "\n",
        "df['tokens'] = df['cleanedText'].apply(lambda x: nltk.word_tokenize(x) if x else [])\n",
        "df['tokensNoStop'] = df['tokens'].apply(removeStopwords)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "df['stemmedTokens'] = df['tokensNoStop'].apply(lambda toks: [stemmer.stem(t) for t in toks])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatizedTokens'] = df['tokensNoStop'].apply(lambda toks: [lemmatizer.lemmatize(t) for t in toks])\n",
        "\n",
        "df.to_csv(\"cleanedProductReviews.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"✅ Cleaned dataset saved as 'cleanedProductReviews.csv'\")\n",
        "\n",
        "print(\"\\nSample (first 5 rows):\")\n",
        "for i in range(min(5, len(df))):\n",
        "    print(\"Original :\", df['reviewText'].iloc[i])\n",
        "    print(\"Cleaned  :\", df['cleanedText'].iloc[i])\n",
        "    print(\"Tokens   :\", df['tokens'].iloc[i])\n",
        "    print(\"NoStop   :\", df['tokensNoStop'].iloc[i])\n",
        "    print(\"Stemmed  :\", df['stemmedTokens'].iloc[i])\n",
        "    print(\"Lemmas   :\", df['lemmatizedTokens'].iloc[i])\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAr2oBL3thoF",
        "outputId": "ba706f12-52bc-48ae-c2ec-17dda669a3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['Review Title', 'Customer name', 'Rating', 'Date', 'Category', 'Comments', 'Useful']\n",
            "Using text column: Review Title\n",
            "✅ Cleaned dataset saved as 'cleanedProductReviews.csv'\n",
            "\n",
            "Sample (first 5 rows):\n",
            "Original : Another Midrange killer Smartphone by Xiaomi\n",
            "Cleaned  : another midrange killer smartphone by xiaomi\n",
            "Tokens   : ['another', 'midrange', 'killer', 'smartphone', 'by', 'xiaomi']\n",
            "NoStop   : ['another', 'midrange', 'killer', 'smartphone', 'xiaomi']\n",
            "Stemmed  : ['anoth', 'midrang', 'killer', 'smartphon', 'xiaomi']\n",
            "Lemmas   : ['another', 'midrange', 'killer', 'smartphone', 'xiaomi']\n",
            "------------------------------------------------------------\n",
            "Original : vry small size mobile\n",
            "Cleaned  : vry small size mobile\n",
            "Tokens   : ['vry', 'small', 'size', 'mobile']\n",
            "NoStop   : ['vry', 'small', 'size', 'mobile']\n",
            "Stemmed  : ['vri', 'small', 'size', 'mobil']\n",
            "Lemmas   : ['vry', 'small', 'size', 'mobile']\n",
            "------------------------------------------------------------\n",
            "Original : Full display not working in all application.\n",
            "Cleaned  : full display not working in all application\n",
            "Tokens   : ['full', 'display', 'not', 'working', 'in', 'all', 'application']\n",
            "NoStop   : ['full', 'display', 'working', 'application']\n",
            "Stemmed  : ['full', 'display', 'work', 'applic']\n",
            "Lemmas   : ['full', 'display', 'working', 'application']\n",
            "------------------------------------------------------------\n",
            "Original : Value for Money\n",
            "Cleaned  : value for money\n",
            "Tokens   : ['value', 'for', 'money']\n",
            "NoStop   : ['value', 'money']\n",
            "Stemmed  : ['valu', 'money']\n",
            "Lemmas   : ['value', 'money']\n",
            "------------------------------------------------------------\n",
            "Original : Not worth for the money\n",
            "Cleaned  : not worth for the money\n",
            "Tokens   : ['not', 'worth', 'for', 'the', 'money']\n",
            "NoStop   : ['worth', 'money']\n",
            "Stemmed  : ['worth', 'money']\n",
            "Lemmas   : ['worth', 'money']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}